{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we compare different runs from a real-world scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_eval import Qrels, Run, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Qrels: 100%|██████████| 5541/5541 [00:00<00:00, 404072.58it/s]\n",
      "Parsing Run: 100%|██████████| 550500/550500 [00:00<00:00, 728397.16it/s]\n",
      "Parsing Run: 100%|██████████| 550500/550500 [00:00<00:00, 853845.99it/s]\n",
      "Parsing Run: 100%|██████████| 550500/550500 [00:00<00:00, 859440.54it/s]\n",
      "Parsing Run: 100%|██████████| 550500/550500 [00:00<00:00, 854441.91it/s]\n",
      "Parsing Run: 100%|██████████| 550500/550500 [00:00<00:00, 742908.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Lets import qrels and runs\n",
    "qrels = Qrels.from_file(\"data/qrels.txt\")\n",
    "\n",
    "run_1 = Run.from_file(\"data/run_1.txt\")\n",
    "run_2 = Run.from_file(\"data/run_2.txt\")\n",
    "run_3 = Run.from_file(\"data/run_3.txt\")\n",
    "run_4 = Run.from_file(\"data/run_4.txt\")\n",
    "run_5 = Run.from_file(\"data/run_5.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# While parsing run files, the run names are compied into the `name` of each Run\n",
    "# These names will be used later on to create tables\n",
    "# You can rename them just by assigning a new value to the attribute\n",
    "run_1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: Custom names can be given to each run, by simply changing the `name` attribute value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets compare all of our runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = compare(\n",
    "    qrels,\n",
    "    runs=[run_1, run_2, run_3, run_4, run_5],\n",
    "    metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n",
    "    max_p=0.01  # P-value threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now print the report of our comparison\n",
    "Superscripts indicates statistical significance in Fisher's Randomization Test with p <= max_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#    Model    MAP@100     MRR@100     NDCG@10\n",
       "---  -------  ----------  ----------  ----------\n",
       "a    model_1  0.3202ᵇ     0.3207ᵇ     0.3684ᵇᶜ\n",
       "b    model_2  0.2332      0.2339      0.239\n",
       "c    model_3  0.3082ᵇ     0.3089ᵇ     0.3295ᵇ\n",
       "d    model_4  0.3664ᵃᵇᶜ   0.3668ᵃᵇᶜ   0.4078ᵃᵇᶜ\n",
       "e    model_5  0.4053ᵃᵇᶜᵈ  0.4061ᵃᵇᶜᵈ  0.4512ᵃᵇᶜᵈ"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_1': {'map@100': 0.3202013385892648,\n",
       "  'mrr@100': 0.3206643638092855,\n",
       "  'ndcg@10': 0.368354920783379},\n",
       " 'model_2': {'map@100': 0.2331730155338538,\n",
       "  'mrr@100': 0.23393300857674673,\n",
       "  'ndcg@10': 0.23899376288638746},\n",
       " 'model_3': {'map@100': 0.30815542768136706,\n",
       "  'mrr@100': 0.308894877341305,\n",
       "  'ndcg@10': 0.32946881898869673},\n",
       " 'model_4': {'map@100': 0.36636770946634367,\n",
       "  'mrr@100': 0.36678226339878717,\n",
       "  'ndcg@10': 0.40777140890090235},\n",
       " 'model_5': {'map@100': 0.40529287119619145,\n",
       "  'mrr@100': 0.40606597777040787,\n",
       "  'ndcg@10': 0.451200401254887}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raw results can be accessed as follows\n",
    "dict(report.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By inspecisting the `win_tie_loss` attribute of the report object, we can get other insights of the model performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'map@100': {'W': 2213, 'T': 1412, 'L': 1880},\n",
       " 'mrr@100': {'W': 2213, 'T': 1420, 'L': 1872},\n",
       " 'ndcg@10': {'W': 1820, 'T': 2191, 'L': 1494}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.win_tie_loss[\"model_5\", \"model_4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we can export the report in LaTeX format, ready to be used in scientific publications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "% Add in preamble\n",
      "\\usepackage{graphicx}\n",
      "\\setlength{\\tabcolsep}{6pt}\n",
      "========================\n",
      "\\begin{table*}[ht]\n",
      "\\centering\n",
      "\\caption{\n",
      "Overall effectiveness of the models.\n",
      "The best results are highlighted in boldface.\n",
      "Superscripts denote significant differences in Fisher's randomization test with $p\\le0.01$.\n",
      "}\n",
      "\\resizebox{1.0\\textwidth}{!}{\n",
      "\\begin{tabular}{c|l|l|l|l}\n",
      "\\toprule\n",
      "\\textbf{\\#}\n",
      "& \\textbf{Model}\n",
      "& \\textbf{MAP@100}\n",
      "& \\textbf{MRR@100}\n",
      "& \\textbf{NDCG@10} \\\\ \n",
      "\\midrule\n",
      "a &\n",
      "model\\_1 &\n",
      "0.3202$^{b}$ &\n",
      "0.3207$^{b}$ &\n",
      "0.3684$^{bc}$ \\\\\n",
      "b &\n",
      "model\\_2 &\n",
      "0.2332 &\n",
      "0.2339 &\n",
      "0.239 \\\\\n",
      "c &\n",
      "model\\_3 &\n",
      "0.3082$^{b}$ &\n",
      "0.3089$^{b}$ &\n",
      "0.3295$^{b}$ \\\\\n",
      "d &\n",
      "model\\_4 &\n",
      "0.3664$^{abc}$ &\n",
      "0.3668$^{abc}$ &\n",
      "0.4078$^{abc}$ \\\\\n",
      "e &\n",
      "model\\_5 &\n",
      "\\textbf{0.4053}$^{abcd}$ &\n",
      "\\textbf{0.4061}$^{abcd}$ &\n",
      "\\textbf{0.4512}$^{abcd}$ \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "}\n",
      "\\label{tab:results}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "print(report.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b69966b56ec652657ae3b55d224973441ad69f336f84cdf432e04c6fe4732776"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('re_dev': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
