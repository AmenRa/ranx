{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#news","title":"\ud83d\udd25 News","text":"<ul> <li>[August 3 2023] <code>ranx</code> <code>0.3.16</code> is out! This release adds support for importing Qrels and Runs from <code>parquet</code> files, exporting them as <code>pandas.DataFrame</code> and save them as <code>parquet</code> files. Any dependence on <code>trec_eval</code> have been removed to make <code>ranx</code> truly MIT-compliant.</li> </ul>"},{"location":"#introduction","title":"\u26a1\ufe0f Introduction","text":"<p>ranx ([ra\u014bks]) is a library of fast ranking evaluation metrics implemented in Python, leveraging Numba for high-speed vector operations and automatic parallelization. It offers a user-friendly interface to evaluate and compare Information Retrieval and Recommender Systems. ranx allows you to perform statistical tests and export LaTeX tables for your scientific publications. Moreover, ranx provides several fusion algorithms and normalization strategies, and an automatic fusion optimization functionality. ranx also have a companion repository of pre-computed runs to facilitated model comparisons called ranxhub. On ranxhub, you can download and share pre-computed runs for Information Retrieval datasets, such as MSMARCO Passage Ranking. ranx was featured in ECIR 2022, CIKM 2022, and SIGIR 2023. </p> <p>If you use ranx to evaluate results or conducting experiments involving fusion for your scientific publication, please consider citing it: evaluation bibtex, fusion bibtex, ranxhub bibtex.</p> <p>NB: <code>ranx</code> is not suited for evaluating classifiers. Please, refer to the FAQ for further details.</p> <p>For a quick overview, follow the Usage section.</p> <p>For a in-depth overview, follow the Examples section.</p>"},{"location":"#features","title":"\u2728 Features","text":""},{"location":"#metrics","title":"Metrics","text":"<ul> <li>Hits</li> <li>Hit Rate</li> <li>Precision</li> <li>Recall</li> <li>F1</li> <li>r-Precision</li> <li>Bpref</li> <li>Rank-biased Precision (RBP)</li> <li>Mean Reciprocal Rank (MRR)</li> <li>Mean Average Precision (MAP)</li> <li>Discounted Cumulative Gain (DCG)</li> <li>Normalized Discounted Cumulative Gain (NDCG)</li> </ul> <p>The metrics have been tested against TREC Eval for correctness.</p>"},{"location":"#statistical-tests","title":"Statistical Tests","text":"<ul> <li>Paired Student's t-Test (default)</li> <li>Fisher's Randomization Test</li> <li>Tukey's HSD Test</li> </ul> <p>Please, refer to Smucker et al., Carterette, and Fuhr for additional information on statistical tests for Information Retrieval.</p>"},{"location":"#off-the-shelf-qrels","title":"Off-the-shelf Qrels","text":"<p>You can load qrels from ir-datasets as simply as: <pre><code>qrels = Qrels.from_ir_datasets(\"msmarco-document/dev\")\n</code></pre> A full list of the available qrels is provided here.</p>"},{"location":"#off-the-shelf-runs","title":"Off-the-shelf Runs","text":"<p>You can load runs from ranxhub as simply as: <pre><code>run = Run.from_ranxhub(\"run-id\")\n</code></pre> A full list of the available runs is provided here.</p>"},{"location":"#fusion-algorithms","title":"Fusion Algorithms","text":"Name Name Name Name Name CombMIN CombMNZ RRF MAPFuse BordaFuse CombMED CombGMNZ RBC PosFuse Weighted BordaFuse CombANZ ISR WMNZ ProbFuse Condorcet CombMAX Log_ISR Mixed SegFuse Weighted Condorcet CombSUM LogN_ISR BayesFuse SlideFuse Weighted Sum <p>Please, refer to the documentation for further details.</p>"},{"location":"#normalization-strategies","title":"Normalization Strategies","text":"<ul> <li>Min-Max Norm </li> <li>Max Norm </li> <li>Sum Norm </li> <li>ZMUV Norm </li> <li>Rank Norm </li> <li>Borda Norm</li> </ul> <p>Please, refer to the documentation for further details.</p>"},{"location":"#requirements","title":"\ud83d\udd0c Requirements","text":"<p><pre><code>python&gt;=3.8\n</code></pre> As of <code>v.0.3.5</code>, ranx requires <code>python&gt;=3.8</code>.</p>"},{"location":"#installation","title":"\ud83d\udcbe Installation","text":"<pre><code>pip install ranx\n</code></pre>"},{"location":"#usage","title":"\ud83d\udca1 Usage","text":""},{"location":"#create-qrels-and-run","title":"Create Qrels and Run","text":"<pre><code>from ranx import Qrels, Run\n\nqrels_dict = { \"q_1\": { \"d_12\": 5, \"d_25\": 3 },\n               \"q_2\": { \"d_11\": 6, \"d_22\": 1 } }\n\nrun_dict = { \"q_1\": { \"d_12\": 0.9, \"d_23\": 0.8, \"d_25\": 0.7,\n                      \"d_36\": 0.6, \"d_32\": 0.5, \"d_35\": 0.4  },\n             \"q_2\": { \"d_12\": 0.9, \"d_11\": 0.8, \"d_25\": 0.7,\n                      \"d_36\": 0.6, \"d_22\": 0.5, \"d_35\": 0.4  } }\n\nqrels = Qrels(qrels_dict)\nrun = Run(run_dict)\n</code></pre>"},{"location":"#evaluate","title":"Evaluate","text":"<pre><code>from ranx import evaluate\n\n# Compute score for a single metric\nevaluate(qrels, run, \"ndcg@5\")\n&gt;&gt;&gt; 0.7861\n\n# Compute scores for multiple metrics at once\nevaluate(qrels, run, [\"map@5\", \"mrr\"])\n&gt;&gt;&gt; {\"map@5\": 0.6416, \"mrr\": 0.75}\n</code></pre>"},{"location":"#compare","title":"Compare","text":"<p><pre><code>from ranx import compare\n\n# Compare different runs and perform Two-sided Paired Student's t-Test\nreport = compare(\n    qrels=qrels,\n    runs=[run_1, run_2, run_3, run_4, run_5],\n    metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n    max_p=0.01  # P-value threshold\n)\n</code></pre> Output: <pre><code>print(report)\n</code></pre> <pre><code>#    Model    MAP@100    MRR@100    NDCG@10\n---  -------  --------   --------   ---------\na    model_1  0.320\u1d47     0.320\u1d47     0.368\u1d47\u1d9c\nb    model_2  0.233      0.234      0.239\nc    model_3  0.308\u1d47     0.309\u1d47     0.330\u1d47\nd    model_4  0.366\u1d43\u1d47\u1d9c   0.367\u1d43\u1d47\u1d9c   0.408\u1d43\u1d47\u1d9c\ne    model_5  0.405\u1d43\u1d47\u1d9c\u1d48  0.406\u1d43\u1d47\u1d9c\u1d48  0.451\u1d43\u1d47\u1d9c\u1d48\n</code></pre></p>"},{"location":"#fusion","title":"Fusion","text":"<pre><code>from ranx import fuse, optimize_fusion\n\nbest_params = optimize_fusion(\n    qrels=train_qrels,\n    runs=[train_run_1, train_run_2, train_run_3],\n    norm=\"min-max\",     # The norm. to apply before fusion\n    method=\"wsum\",      # The fusion algorithm to use (Weighted Sum)\n    metric=\"ndcg@100\",  # The metric to maximize\n)\n\ncombined_test_run = fuse(\n    runs=[test_run_1, test_run_2, test_run_3],  \n    norm=\"min-max\",       \n    method=\"wsum\",        \n    params=best_params,\n)\n</code></pre>"},{"location":"#examples","title":"\ud83d\udcd6 Examples","text":"Name Link Overview Qrels and Run Evaluation Comparison and Report Fusion Plot Share your runs with ranxhub"},{"location":"#documentation","title":"\ud83d\udcda Documentation","text":"<p>Browse the documentation for more details and examples.</p>"},{"location":"#citation","title":"\ud83c\udf93 Citation","text":"<p>If you use ranx to evaluate results for your scientific publication, please consider citing our ECIR 2022 paper:</p> BibTeX <pre><code>@inproceedings{ranx,\n  author       = {Elias Bassani},\n  title        = {ranx: {A} Blazing-Fast Python Library for Ranking Evaluation and Comparison},\n  booktitle    = {{ECIR} {(2)}},\n  series       = {Lecture Notes in Computer Science},\n  volume       = {13186},\n  pages        = {259--264},\n  publisher    = {Springer},\n  year         = {2022},\n  doi          = {10.1007/978-3-030-99739-7\\_30}\n}\n</code></pre> <p>If you use the fusion functionalities provided by ranx for conducting the experiments of your scientific publication, please consider citing our CIKM 2022 paper:</p> BibTeX <pre><code>@inproceedings{ranx.fuse,\n  author    = {Elias Bassani and\n              Luca Romelli},\n  title     = {ranx.fuse: {A} Python Library for Metasearch},\n  booktitle = {{CIKM}},\n  pages     = {4808--4812},\n  publisher = {{ACM}},\n  year      = {2022},\n  doi       = {10.1145/3511808.3557207}\n}\n</code></pre> <p>If you use pre-computed runs from [ranxhub]((https://amenra.github.io/ranxhub) to make comparison for your scientific publication, please consider citing our SIGIR 2023 paper:</p> BibTeX <pre><code>@inproceedings{ranxhub,\n  author       = {Elias Bassani},\n  title        = {ranxhub: An Online Repository for Information Retrieval Runs},\n  booktitle    = {{SIGIR}},\n  pages        = {3210--3214},\n  publisher    = {{ACM}},\n  year         = {2023},\n  doi          = {10.1145/3539618.3591823}\n}\n</code></pre>"},{"location":"#feature-requests","title":"\ud83c\udf81 Feature Requests","text":"<p>Would you like to see other features implemented? Please, open a feature request.</p>"},{"location":"#want-to-contribute","title":"\ud83e\udd18 Want to contribute?","text":"<p>Would you like to contribute? Please, drop me an e-mail.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>ranx is an open-sourced software licensed under the MIT license.</p>"},{"location":"compare/","title":"Compare","text":""},{"location":"compare/#ranx.compare.compare","title":"<code>compare(qrels, runs, metrics, stat_test='student', n_permutations=1000, max_p=0.01, random_seed=42, threads=0, rounding_digits=3, show_percentages=False, make_comparable=False)</code>","text":"<p>Evaluate multiple <code>runs</code> and compute statistical tests.</p> <p>Usage example: <pre><code>from ranx import compare\n\n# Compare different runs and perform statistical tests\nreport = compare(\n    qrels=qrels,\n    runs=[run_1, run_2, run_3, run_4, run_5],\n    metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n    max_p=0.01  # P-value threshold\n)\n\nprint(report)\n</code></pre> Output: <pre><code>#    Model    MAP@100     MRR@100     NDCG@10\n---  -------  ----------  ----------  ----------\na    model_1  0.3202\u1d47     0.3207\u1d47     0.3684\u1d47\u1d9c\nb    model_2  0.2332      0.2339      0.239\nc    model_3  0.3082\u1d47     0.3089\u1d47     0.3295\u1d47\nd    model_4  0.3664\u1d43\u1d47\u1d9c   0.3668\u1d43\u1d47\u1d9c   0.4078\u1d43\u1d47\u1d9c\ne    model_5  0.4053\u1d43\u1d47\u1d9c\u1d48  0.4061\u1d43\u1d47\u1d9c\u1d48  0.4512\u1d43\u1d47\u1d9c\u1d48\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>qrels</code> <code>Qrels</code> <p>Qrels.</p> required <code>runs</code> <code>List[Run]</code> <p>List of runs.</p> required <code>metrics</code> <code>Union[List[str], str]</code> <p>Metric or list of metrics.</p> required <code>n_permutations</code> <code>int</code> <p>Number of permutation to perform during statistical testing (Fisher's Randomization Test is used by default). Defaults to 1000.</p> <code>1000</code> <code>max_p</code> <code>float</code> <p>Maximum p-value to consider an increment as statistically significant. Defaults to 0.01.</p> <code>0.01</code> <code>stat_test</code> <code>str</code> <p>Statistical test to perform. Use \"fisher\" for Fisher's Randomization Test, \"student\" for Two-sided Paired Student's t-Test, or \"Tukey\" for Tukey's HSD test. Defaults to \"student\".</p> <code>'student'</code> <code>random_seed</code> <code>int</code> <p>Random seed to use for generating the permutations. Defaults to 42.</p> <code>42</code> <code>threads</code> <code>int</code> <p>Number of threads to use, zero means all the available threads. Defaults to 0.</p> <code>0</code> <code>rounding_digits</code> <code>int</code> <p>Number of digits to round to and to show in the Report. Defaults to 3.</p> <code>3</code> <code>show_percentages</code> <code>bool</code> <p>Whether to show percentages instead of floats in the Report. Defaults to False.</p> <code>False</code> <code>make_comparable</code> <code>bool</code> <p>Adds empty results for queries missing from the runs and removes those not appearing in qrels. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Report</code> <code>Report</code> <p>See report.</p> Source code in <code>ranx/meta/compare.py</code> <pre><code>def compare(\n    qrels: Qrels,\n    runs: List[Run],\n    metrics: Union[List[str], str],\n    stat_test: str = \"student\",\n    n_permutations: int = 1000,\n    max_p: float = 0.01,\n    random_seed: int = 42,\n    threads: int = 0,\n    rounding_digits: int = 3,\n    show_percentages: bool = False,\n    make_comparable: bool = False,\n) -&gt; Report:\n    \"\"\"Evaluate multiple `runs` and compute statistical tests.\n\n    Usage example:\n    ```python\n    from ranx import compare\n\n    # Compare different runs and perform statistical tests\n    report = compare(\n        qrels=qrels,\n        runs=[run_1, run_2, run_3, run_4, run_5],\n        metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n        max_p=0.01  # P-value threshold\n    )\n\n    print(report)\n    ```\n    Output:\n    ```\n    #    Model    MAP@100     MRR@100     NDCG@10\n    ---  -------  ----------  ----------  ----------\n    a    model_1  0.3202\u1d47     0.3207\u1d47     0.3684\u1d47\u1d9c\n    b    model_2  0.2332      0.2339      0.239\n    c    model_3  0.3082\u1d47     0.3089\u1d47     0.3295\u1d47\n    d    model_4  0.3664\u1d43\u1d47\u1d9c   0.3668\u1d43\u1d47\u1d9c   0.4078\u1d43\u1d47\u1d9c\n    e    model_5  0.4053\u1d43\u1d47\u1d9c\u1d48  0.4061\u1d43\u1d47\u1d9c\u1d48  0.4512\u1d43\u1d47\u1d9c\u1d48\n    ```\n\n    Args:\n        qrels (Qrels): Qrels.\n        runs (List[Run]): List of runs.\n        metrics (Union[List[str], str]): Metric or list of metrics.\n        n_permutations (int, optional): Number of permutation to perform during statistical testing (Fisher's Randomization Test is used by default). Defaults to 1000.\n        max_p (float, optional): Maximum p-value to consider an increment as statistically significant. Defaults to 0.01.\n        stat_test (str, optional): Statistical test to perform. Use \"fisher\" for _Fisher's Randomization Test_, \"student\" for _Two-sided Paired Student's t-Test_, or \"Tukey\" for _Tukey's HSD test_. Defaults to \"student\".\n        random_seed (int, optional): Random seed to use for generating the permutations. Defaults to 42.\n        threads (int, optional): Number of threads to use, zero means all the available threads. Defaults to 0.\n        rounding_digits (int, optional): Number of digits to round to and to show in the Report. Defaults to 3.\n        show_percentages (bool, optional): Whether to show percentages instead of floats in the Report. Defaults to False.\n        make_comparable (bool, optional): Adds empty results for queries missing from the runs and removes those not appearing in qrels. Defaults to False.\n\n    Returns:\n        Report: See report.\n    \"\"\"\n    metrics = format_metrics(metrics)\n    assert all(isinstance(m, str) for m in metrics), \"Metrics error\"\n\n    model_names = []\n    results = defaultdict(dict)\n\n    metric_scores = {}\n\n    # Compute scores for each run for each query -------------------------------\n    for i, run in enumerate(runs):\n        model_name = run.name if run.name is not None else f\"run_{i+1}\"\n        model_names.append(model_name)\n\n        metric_scores[model_name] = evaluate(\n            qrels=qrels,\n            run=run,\n            metrics=metrics,\n            return_mean=False,\n            threads=threads,\n            make_comparable=make_comparable,\n        )\n\n        if len(metrics) == 1:\n            metric_scores[model_name] = {metrics[0]: metric_scores[model_name]}\n\n        for m in metrics:\n            results[model_name][m] = float(np.mean(metric_scores[model_name][m]))\n\n    # Run statistical testing --------------------------------------------------\n    comparisons = compute_statistical_significance(\n        model_names=model_names,\n        metric_scores=metric_scores,\n        stat_test=stat_test,\n        n_permutations=n_permutations,\n        max_p=max_p,\n        random_seed=random_seed,\n    )\n\n    # Compute win / tie / lose -------------------------------------------------\n    win_tie_loss = defaultdict(dict)\n\n    for control in model_names:\n        for treatment in model_names:\n            if control != treatment:\n                for m in metrics:\n                    control_scores = metric_scores[control][m]\n                    treatment_scores = metric_scores[treatment][m]\n                    win_tie_loss[(control, treatment)][m] = {\n                        \"W\": int(sum(control_scores &gt; treatment_scores)),\n                        \"T\": int(sum(control_scores == treatment_scores)),\n                        \"L\": int(sum(control_scores &lt; treatment_scores)),\n                    }\n\n    return Report(\n        model_names=model_names,\n        results=dict(results),\n        comparisons=comparisons,\n        metrics=metrics,\n        max_p=max_p,\n        win_tie_loss=dict(win_tie_loss),\n        rounding_digits=rounding_digits,\n        show_percentages=show_percentages,\n        stat_test=stat_test,\n    )\n</code></pre>"},{"location":"evaluate/","title":"Evaluate","text":""},{"location":"evaluate/#ranx.evaluate.evaluate","title":"<code>evaluate(qrels, run, metrics, return_mean=True, return_std=False, threads=0, save_results_in_run=True, make_comparable=False)</code>","text":"<p>Compute the performance scores for the provided <code>qrels</code> and <code>run</code> for all the specified metrics.</p> <p>Usage examples:</p> <p>from ranx import evaluate</p>"},{"location":"evaluate/#ranx.evaluate.evaluate--compute-score-for-a-single-metric","title":"Compute score for a single metric","text":"<p>evaluate(qrels, run, \"ndcg@5\")</p> <p>0.7861</p>"},{"location":"evaluate/#ranx.evaluate.evaluate--compute-scores-for-multiple-metrics-at-once","title":"Compute scores for multiple metrics at once","text":"<p>evaluate(qrels, run, [\"map@5\", \"mrr\"])</p> <p>{\"map@5\": 0.6416, \"mrr\": 0.75}</p>"},{"location":"evaluate/#ranx.evaluate.evaluate--computed-metric-scores-are-saved-in-the-run-object","title":"Computed metric scores are saved in the Run object","text":"<p>run.mean_scores</p> <p>{\"ndcg@5\": 0.7861, \"map@5\": 0.6416, \"mrr\": 0.75}</p>"},{"location":"evaluate/#ranx.evaluate.evaluate--access-scores-for-each-query","title":"Access scores for each query","text":"<p>dict(run.scores)</p> <p>{ ...     \"ndcg@5\": {\"q_1\": 0.9430, \"q_2\": 0.6292}, ...     \"map@5\": {\"q_1\": 0.8333, \"q_2\": 0.4500}, ...     \"mrr\": {\"q_1\": 1.0000, \"q_2\": 0.5000}, ... } Args:     qrels (Union[ Qrels, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Qrels.     run (Union[ Run, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Run.     metrics (Union[List[str], str]): Metrics or list of metric to compute.     return_mean (bool, optional): Whether to return the metric scores averaged over the query set or the scores for individual queries. Defaults to True.     threads (int, optional): Number of threads to use, zero means all the available threads. Defaults to 0.     save_results_in_run (bool, optional): Save metric scores for each query in the input <code>run</code>. Defaults to True.     make_comparable (bool, optional): Adds empty results for queries missing from the run and removes those not appearing in qrels. Defaults to False.</p> <p>Returns:</p> Type Description <code>Union[Dict[str, float], float]</code> <p>Union[Dict[str, float], float]: Results.</p> Source code in <code>ranx/meta/evaluate.py</code> <pre><code>def evaluate(\n    qrels: Union[\n        Qrels,\n        Dict[str, Dict[str, Number]],\n        nb.typed.typedlist.List,\n        np.ndarray,\n    ],\n    run: Union[\n        Run,\n        Dict[str, Dict[str, Number]],\n        nb.typed.typedlist.List,\n        np.ndarray,\n    ],\n    metrics: Union[List[str], str],\n    return_mean: bool = True,\n    return_std: bool = False,\n    threads: int = 0,\n    save_results_in_run: bool = True,\n    make_comparable: bool = False,\n) -&gt; Union[Dict[str, float], float]:\n    \"\"\"Compute the performance scores for the provided `qrels` and `run` for all the specified metrics.\n\n    Usage examples:\n\n    from ranx import evaluate\n\n    # Compute score for a single metric\n    evaluate(qrels, run, \"ndcg@5\")\n    &gt;&gt;&gt; 0.7861\n\n    # Compute scores for multiple metrics at once\n    evaluate(qrels, run, [\"map@5\", \"mrr\"])\n    &gt;&gt;&gt; {\"map@5\": 0.6416, \"mrr\": 0.75}\n\n    # Computed metric scores are saved in the Run object\n    run.mean_scores\n    &gt;&gt;&gt; {\"ndcg@5\": 0.7861, \"map@5\": 0.6416, \"mrr\": 0.75}\n\n    # Access scores for each query\n    dict(run.scores)\n    &gt;&gt;&gt; {\n    ...     \"ndcg@5\": {\"q_1\": 0.9430, \"q_2\": 0.6292},\n    ...     \"map@5\": {\"q_1\": 0.8333, \"q_2\": 0.4500},\n    ...     \"mrr\": {\"q_1\": 1.0000, \"q_2\": 0.5000},\n    ... }\n    Args:\n        qrels (Union[ Qrels, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Qrels.\n        run (Union[ Run, Dict[str, Dict[str, Number]], nb.typed.typedlist.List, np.ndarray, ]): Run.\n        metrics (Union[List[str], str]): Metrics or list of metric to compute.\n        return_mean (bool, optional): Whether to return the metric scores averaged over the query set or the scores for individual queries. Defaults to True.\n        threads (int, optional): Number of threads to use, zero means all the available threads. Defaults to 0.\n        save_results_in_run (bool, optional): Save metric scores for each query in the input `run`. Defaults to True.\n        make_comparable (bool, optional): Adds empty results for queries missing from the run and removes those not appearing in qrels. Defaults to False.\n\n    Returns:\n        Union[Dict[str, float], float]: Results.\n    \"\"\"\n\n    if len(qrels) &lt; 10:\n        set_num_threads(1)\n    elif threads != 0:\n        set_num_threads(threads)\n\n    if not return_mean:\n        return_std = False\n\n    if make_comparable and type(qrels) == Qrels and type(run) == Run:\n        run = run.make_comparable(qrels)\n\n    if type(qrels) in [Qrels, dict] and type(run) in [Run, dict]:\n        check_keys(qrels, run)\n\n    _qrels = convert_qrels(qrels)\n    _run = convert_run(run)\n    metrics = format_metrics(metrics)\n    assert all(isinstance(m, str) for m in metrics), \"Metrics error\"\n\n    # Compute metrics ----------------------------------------------------------\n    metric_scores_dict = {}\n    for metric in metrics:\n        m, k, rel_lvl = extract_metric_and_params(metric)\n        metric_scores_dict[metric] = metric_switch(m)(_qrels, _run, k, rel_lvl)\n\n    # Save results in Run ------------------------------------------------------\n    if type(run) == Run and save_results_in_run:\n        for m, scores in metric_scores_dict.items():\n            run.mean_scores[m] = np.mean(scores)\n            if return_std:\n                run.std_scores[m] = np.std(scores)\n            for i, q_id in enumerate(run.get_query_ids()):\n                run.scores[m][q_id] = scores[i]\n\n    # Prepare output -----------------------------------------------------------\n    if return_mean:\n        for m, scores in metric_scores_dict.items():\n            if return_std:\n                metric_scores_dict[m] = {\n                    \"mean\": np.mean(scores),\n                    \"std\": np.std(scores),\n                }\n\n            else:\n                metric_scores_dict[m] = np.mean(scores)\n\n    return metric_scores_dict[m] if len(metrics) == 1 else metric_scores_dict\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#is-ranx-suited-for-evaluating-classification-tasks","title":"Is <code>ranx</code> suited for evaluating classification tasks?","text":"<p>No, it's not. <code>ranx</code> is meant for ranking tasks. Although some metrics are commonly used for evaluation of both tasks (e.g., <code>precision</code> and <code>recall</code>) the relevance scores stored in <code>runs</code> should not be confused with the predicted class labels of a classification task. Relevance scores are used by <code>ranx</code> to sort results before computing the metrics, regardless of their actual values.</p>"},{"location":"faq/#are-zero-and-negative-scored-results-filtered-out-by-ranx","title":"Are zero and negative scored results filtered out by <code>ranx</code>?","text":"<p>Zero and negative scored results are NOT filtered out by <code>ranx</code>. Relevance scores are used only for sorting and there is no constraint on the values produce by a ranking models, although some of them only outputs positive values. Therefore, if you think that zero and negative scored results should be filtered out, you should do it before passing the <code>runs</code> to <code>ranx</code>.</p>"},{"location":"fusion/","title":"Fusion","text":""},{"location":"fusion/#fuse","title":"Fuse","text":"<p><code>ranx</code> provides several fusion algorithms, all of which can be accessed through a single function in the same fashion as <code>evaluate</code>.</p> <p>Usage example: <pre><code>from ranx import fuse\n\ncombined_run = fuse(\n    runs=[run_1, run_2],  # A list of Run instances to fuse\n    norm=\"min-max\",       # The normalization strategy to apply before fusion\n    method=\"max\",         # The fusion algorithm to use \n)\n</code></pre></p>"},{"location":"fusion/#optimize-fusion","title":"Optimize Fusion","text":"<p>As many fusion algorithms require a training or optimization step, <code>ranx</code> provides a function to optimize all of those algorithms. For algorithms requiring hyper-parameter optimization, <code>ranx</code> automatically evaluates pre-defined configurations via grid search. In those cases, <code>ranx</code> shows a progress bar.</p> <p>Usage example: <pre><code>from ranx import fuse, optimize_fusion\n\nbest_params = optimize_fusion(\n    qrels=train_qrels,\n    runs=[train_run_1, train_run_2, train_run_3],\n    norm=\"min-max\",\n    method=\"wsum\",\n    metric=\"ndcg@100\",  # The metric to maximize during optimization\n)\n\ncombined_test_run = fuse(\n    runs=[test_run_1, test_run_2, test_run_3],  \n    norm=\"min-max\",       \n    method=\"wsum\",        \n    params=best_params,\n)\n</code></pre></p>"},{"location":"fusion/#supported-fusion-algorithms","title":"Supported fusion algorithms","text":"<p><code>ranx</code> supports the following fusion algorithms:</p> Algorithm Alias Optim. Algorithm Alias Optim. CombMIN min No CombMAX max No CombMED med No CombSUM sum No CombANZ anz No CombMNZ mnz No CombGMNZ gmnz Yes ISR isr No Log_ISR log_isr No LogN_ISR logn_isr Yes Reciprocal Rank Fusion (RRF) rrf Yes PosFuse posfuse Yes ProbFuse probfuse Yes SegFuse segfuse Yes SlideFuse slidefuse Yes MAPFuse mapfuse Yes BordaFuse bordafuse No Weighted BordaFuse w_bordafuse Yes Condorcet condorcet No Weighted Condorcet w_condorcet Yes BayesFuse bayesfuse Yes Mixed mixed Yes WMNZ wmnz Yes Weighted Sum wsum Yes Rank-Biased Centroids (RBC) rbc Yes"},{"location":"fusion/#bayesfuse","title":"BayesFuse","text":"<p>Computes BayesFuse as proposed by Aslam et al..  </p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/AslamM01,\n    author    = {Javed A. Aslam and\n                Mark H. Montague},\n    editor    = {W. Bruce Croft and\n                David J. Harper and\n                Donald H. Kraft and\n                Justin Zobel},\n    title     = {Models for Metasearch},\n    booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR}\n                Conference on Research and Development in Information Retrieval, September\n                9-13, 2001, New Orleans, Louisiana, {USA}},\n    pages     = {275--284},\n    publisher = {{ACM}},\n    year      = {2001},\n    url       = {https://doi.org/10.1145/383952.384007},\n    doi       = {10.1145/383952.384007},\n    timestamp = {Tue, 06 Nov 2018 11:07:25 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/AslamM01.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#bordafuse","title":"BordaFuse","text":"<p>Computes BordaFuse as proposed by Aslam et al..  </p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/AslamM01,\n    author    = {Javed A. Aslam and\n                Mark H. Montague},\n    editor    = {W. Bruce Croft and\n                David J. Harper and\n                Donald H. Kraft and\n                Justin Zobel},\n    title     = {Models for Metasearch},\n    booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR}\n                Conference on Research and Development in Information Retrieval, September\n                9-13, 2001, New Orleans, Louisiana, {USA}},\n    pages     = {275--284},\n    publisher = {{ACM}},\n    year      = {2001},\n    url       = {https://doi.org/10.1145/383952.384007},\n    doi       = {10.1145/383952.384007},\n    timestamp = {Tue, 06 Nov 2018 11:07:25 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/AslamM01.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#combanz","title":"CombANZ","text":"<p>Computes CombANZ as proposed by Fox et al..  </p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#combgmnz","title":"CombGMNZ","text":"<p>Computes CombGMNZ as proposed by Joon Ho Lee.</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/Lee97,\n    author    = {Joon Ho Lee},\n    title     = {Analyses of Multiple Evidence Combination},\n    booktitle = {{SIGIR}},\n    pages     = {267--276},\n    publisher = {{ACM}},\n    year      = {1997}\n}\n</code></pre> Optimization Parameter Default Value min_gamma 0.01 max_gamma 1.0 step 0.01"},{"location":"fusion/#combmax","title":"CombMAX","text":"<p>Computes CombMAX as proposed by Fox et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#combmed","title":"CombMED","text":"<p>Computes CombMED as proposed by Fox et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#combmin","title":"CombMIN","text":"<p>Computes CombMIN as proposed by Fox et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#combmnz","title":"CombMNZ","text":"<p>Computes CombMNZ as proposed by Fox et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#combsum","title":"CombSUM","text":"<p>Computes CombSUM as proposed by Fox et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/trec/FoxS93,\n    author    = {Edward A. Fox and\n                Joseph A. Shaw},\n    title     = {Combination of Multiple Searches},\n    booktitle = {{TREC}},\n    series    = {{NIST} Special Publication},\n    volume    = {500-215},\n    pages     = {243--252},\n    publisher = {National Institute of Standards and Technology {(NIST)}},\n    year      = {1993}\n}\n</code></pre>"},{"location":"fusion/#condorcet","title":"Condorcet","text":"<p>Computes Condorcet as proposed by Montague et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/cikm/MontagueA02,\n    author    = {Mark H. Montague and\n                Javed A. Aslam},\n    title     = {Condorcet fusion for improved retrieval},\n    booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information\n                and Knowledge Management, McLean, VA, USA, November 4-9, 2002},\n    pages     = {538--548},\n    publisher = {{ACM}},\n    year      = {2002},\n    url       = {https://doi.org/10.1145/584792.584881},\n    doi       = {10.1145/584792.584881},\n    timestamp = {Tue, 06 Nov 2018 16:57:50 +0100},\n    biburl    = {https://dblp.org/rec/conf/cikm/MontagueA02.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#isr","title":"ISR","text":"<p>Computes ISR as proposed by Mour\u00e3o et al..</p> BibTeX <pre><code>@article{DBLP:journals/cmig/MouraoMM15,\n    author    = {Andr{\\'{e}} Mour{\\~{a}}o and\n                Fl{\\'{a}}vio Martins and\n                Jo{\\~{a}}o Magalh{\\~{a}}es},\n    title     = {Multimodal medical information retrieval with unsupervised rank fusion},\n    journal   = {Comput. Medical Imaging Graph.},\n    volume    = {39},\n    pages     = {35--45},\n    year      = {2015},\n    url       = {https://doi.org/10.1016/j.compmedimag.2014.05.006},\n    doi       = {10.1016/j.compmedimag.2014.05.006},\n    timestamp = {Thu, 14 May 2020 10:17:16 +0200},\n    biburl    = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#log_isr","title":"Log_ISR","text":"<p>Computes Log_ISR as proposed by Mour\u00e3o et al..</p> BibTeX <pre><code>@article{DBLP:journals/cmig/MouraoMM15,\n    author    = {Andr{\\'{e}} Mour{\\~{a}}o and\n                Fl{\\'{a}}vio Martins and\n                Jo{\\~{a}}o Magalh{\\~{a}}es},\n    title     = {Multimodal medical information retrieval with unsupervised rank fusion},\n    journal   = {Comput. Medical Imaging Graph.},\n    volume    = {39},\n    pages     = {35--45},\n    year      = {2015},\n    url       = {https://doi.org/10.1016/j.compmedimag.2014.05.006},\n    doi       = {10.1016/j.compmedimag.2014.05.006},\n    timestamp = {Thu, 14 May 2020 10:17:16 +0200},\n    biburl    = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#logn_isr","title":"LogN_ISR","text":"<p>Computes Log_ISR as proposed by Mour\u00e3o et al..</p> BibTeX <pre><code>@article{DBLP:journals/cmig/MouraoMM15,\n    author    = {Andr{\\'{e}} Mour{\\~{a}}o and\n                Fl{\\'{a}}vio Martins and\n                Jo{\\~{a}}o Magalh{\\~{a}}es},\n    title     = {Multimodal medical information retrieval with unsupervised rank fusion},\n    journal   = {Comput. Medical Imaging Graph.},\n    volume    = {39},\n    pages     = {35--45},\n    year      = {2015},\n    url       = {https://doi.org/10.1016/j.compmedimag.2014.05.006},\n    doi       = {10.1016/j.compmedimag.2014.05.006},\n    timestamp = {Thu, 14 May 2020 10:17:16 +0200},\n    biburl    = {https://dblp.org/rec/journals/cmig/MouraoMM15.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value min_sigma 0.01 max_sigma 1.0 step 0.01"},{"location":"fusion/#mapfuse","title":"MAPFuse","text":"<p>Computes MAPFuse as proposed by Lillis et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/LillisZTCLD10,\n    author    = {David Lillis and\n                Lusheng Zhang and\n                Fergus Toolan and\n                Rem W. Collier and\n                David Leonard and\n                John Dunnion},\n    editor    = {Fabio Crestani and\n                St{\\'{e}}phane Marchand{-}Maillet and\n                Hsin{-}Hsi Chen and\n                Efthimis N. Efthimiadis and\n                Jacques Savoy},\n    title     = {Estimating probabilities for effective data fusion},\n    booktitle = {Proceeding of the 33rd International {ACM} {SIGIR} Conference on Research\n                and Development in Information Retrieval, {SIGIR} 2010, Geneva, Switzerland,\n                July 19-23, 2010},\n    pages     = {347--354},\n    publisher = {{ACM}},\n    year      = {2010},\n    url       = {https://doi.org/10.1145/1835449.1835508},\n    doi       = {10.1145/1835449.1835508},\n    timestamp = {Tue, 06 Nov 2018 11:07:25 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/LillisZTCLD10.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#mixed","title":"Mixed","text":"<p>Computes Mixed as proposed by Wu et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/cikm/WuC02,\n    author    = {Shengli Wu and\n                Fabio Crestani},\n    title     = {Data fusion with estimated weights},\n    booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information\n                and Knowledge Management, McLean, VA, USA, November 4-9, 2002},\n    pages     = {648--651},\n    publisher = {{ACM}},\n    year      = {2002},\n    url       = {https://doi.org/10.1145/584792.584908},\n    doi       = {10.1145/584792.584908},\n    timestamp = {Tue, 06 Nov 2018 16:57:40 +0100},\n    biburl    = {https://dblp.org/rec/conf/cikm/WuC02.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value step 0.1"},{"location":"fusion/#posfuse","title":"PosFuse","text":"<p>Computes PosFuse as proposed by Lillis et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/LillisZTCLD10,\n    author    = {David Lillis and\n                Lusheng Zhang and\n                Fergus Toolan and\n                Rem W. Collier and\n                David Leonard and\n                John Dunnion},\n    editor    = {Fabio Crestani and\n                St{\\'{e}}phane Marchand{-}Maillet and\n                Hsin{-}Hsi Chen and\n                Efthimis N. Efthimiadis and\n                Jacques Savoy},\n    title     = {Estimating probabilities for effective data fusion},\n    booktitle = {Proceeding of the 33rd International {ACM} {SIGIR} Conference on Research\n                and Development in Information Retrieval, {SIGIR} 2010, Geneva, Switzerland,\n                July 19-23, 2010},\n    pages     = {347--354},\n    publisher = {{ACM}},\n    year      = {2010},\n    url       = {https://doi.org/10.1145/1835449.1835508},\n    doi       = {10.1145/1835449.1835508},\n    timestamp = {Tue, 06 Nov 2018 11:07:25 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/LillisZTCLD10.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#probfuse","title":"ProbFuse","text":"<p>Computes ProbFuse as proposed by Lillis et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/LillisTCD06,\n    author    = {David Lillis and\n                Fergus Toolan and\n                Rem W. Collier and\n                John Dunnion},\n    editor    = {Efthimis N. Efthimiadis and\n                Susan T. Dumais and\n                David Hawking and\n                Kalervo J{\\\"{a}}rvelin},\n    title     = {ProbFuse: a probabilistic approach to data fusion},\n    booktitle = {{SIGIR} 2006: Proceedings of the 29th Annual International {ACM} {SIGIR}\n                Conference on Research and Development in Information Retrieval, Seattle,\n                Washington, USA, August 6-11, 2006},\n    pages     = {139--146},\n    publisher = {{ACM}},\n    year      = {2006},\n    url       = {https://doi.org/10.1145/1148170.1148197},\n    doi       = {10.1145/1148170.1148197},\n    timestamp = {Wed, 14 Nov 2018 10:58:10 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/LillisTCD06.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value min_n_segments 1 max_n_segments 100"},{"location":"fusion/#rank-biased-centroids-rbc","title":"Rank-Biased Centroids (RBC)","text":"<p>Computes Rank-Biased Centroid (RBC) as proposed by Bailey et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/BaileyMST17,\n    author    = {Peter Bailey and\n                Alistair Moffat and\n                Falk Scholer and\n                Paul Thomas},\n    editor    = {Noriko Kando and\n                Tetsuya Sakai and\n                Hideo Joho and\n                Hang Li and\n                Arjen P. de Vries and\n                Ryen W. White},\n    title     = {Retrieval Consistency in the Presence of Query Variations},\n    booktitle = {Proceedings of the 40th International {ACM} {SIGIR} Conference on\n                Research and Development in Information Retrieval, Shinjuku, Tokyo,\n                Japan, August 7-11, 2017},\n    pages     = {395--404},\n    publisher = {{ACM}},\n    year      = {2017},\n    url       = {https://doi.org/10.1145/3077136.3080839},\n    doi       = {10.1145/3077136.3080839},\n    timestamp = {Wed, 25 Sep 2019 16:43:14 +0200},\n    biburl    = {https://dblp.org/rec/conf/sigir/BaileyMST17.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value min_phi 0.01 max_phi 1.0 step 0.01"},{"location":"fusion/#reciprocal-rank-fusion-rrf","title":"Reciprocal Rank Fusion (RRF)","text":"<p>Computes Reciprocal Rank Fusion as proposed by Cormack et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/CormackCB09,\n    author    = {Gordon V. Cormack and\n                Charles L. A. Clarke and\n                Stefan B{\\\"{u}}ttcher},\n    title     = {Reciprocal rank fusion outperforms condorcet and individual rank learning\n                methods},\n    booktitle = {{SIGIR}},\n    pages     = {758--759},\n    publisher = {{ACM}},\n    year      = {2009}\n}\n</code></pre> Optimization Parameter Default Value min_k 10 max_k 100 step 10"},{"location":"fusion/#segfuse","title":"SegFuse","text":"<p>Computes SegFuse as proposed by Shokouhi.</p> BibTeX <pre><code>@inproceedings{DBLP:conf/ecir/Shokouhi07a,\n    author    = {Milad Shokouhi},\n    editor    = {Giambattista Amati and\n                Claudio Carpineto and\n                Giovanni Romano},\n    title     = {Segmentation of Search Engine Results for Effective Data-Fusion},\n    booktitle = {Advances in Information Retrieval, 29th European Conference on {IR}\n                Research, {ECIR} 2007, Rome, Italy, April 2-5, 2007, Proceedings},\n    series    = {Lecture Notes in Computer Science},\n    volume    = {4425},\n    pages     = {185--197},\n    publisher = {Springer},\n    year      = {2007},\n    url       = {https://doi.org/10.1007/978-3-540-71496-5\\_19},\n    doi       = {10.1007/978-3-540-71496-5\\_19},\n    timestamp = {Tue, 14 May 2019 10:00:37 +0200},\n    biburl    = {https://dblp.org/rec/conf/ecir/Shokouhi07a.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre>"},{"location":"fusion/#slidefuse","title":"SlideFuse","text":"<p>Computes SlideFuse as proposed by Lillis et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/ecir/LillisTCD08,\n    author    = {David Lillis and\n                Fergus Toolan and\n                Rem W. Collier and\n                John Dunnion},\n    editor    = {Craig Macdonald and\n                Iadh Ounis and\n                Vassilis Plachouras and\n                Ian Ruthven and\n                Ryen W. White},\n    title     = {Extending Probabilistic Data Fusion Using Sliding Windows},\n    booktitle = {Advances in Information Retrieval , 30th European Conference on {IR}\n                Research, {ECIR} 2008, Glasgow, UK, March 30-April 3, 2008. Proceedings},\n    series    = {Lecture Notes in Computer Science},\n    volume    = {4956},\n    pages     = {358--369},\n    publisher = {Springer},\n    year      = {2008},\n    url       = {https://doi.org/10.1007/978-3-540-78646-7\\_33},\n    doi       = {10.1007/978-3-540-78646-7\\_33},\n    timestamp = {Sun, 25 Oct 2020 22:33:08 +0100},\n    biburl    = {https://dblp.org/rec/conf/ecir/LillisTCD08.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value min_w 1 max_w 100"},{"location":"fusion/#weighted-bordafuse","title":"Weighted BordaFuse","text":"<p>Computes Weighted BordaFuse as proposed by Aslam et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/sigir/AslamM01,\n    author    = {Javed A. Aslam and\n                Mark H. Montague},\n    editor    = {W. Bruce Croft and\n                David J. Harper and\n                Donald H. Kraft and\n                Justin Zobel},\n    title     = {Models for Metasearch},\n    booktitle = {{SIGIR} 2001: Proceedings of the 24th Annual International {ACM} {SIGIR}\n                Conference on Research and Development in Information Retrieval, September\n                9-13, 2001, New Orleans, Louisiana, {USA}},\n    pages     = {275--284},\n    publisher = {{ACM}},\n    year      = {2001},\n    url       = {https://doi.org/10.1145/383952.384007},\n    doi       = {10.1145/383952.384007},\n    timestamp = {Tue, 06 Nov 2018 11:07:25 +0100},\n    biburl    = {https://dblp.org/rec/conf/sigir/AslamM01.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value step 0.1"},{"location":"fusion/#weighted-condorcet","title":"Weighted Condorcet","text":"<p>Computes Weighted Condorcet as proposed by Montague et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/cikm/MontagueA02,\n    author    = {Mark H. Montague and\n                Javed A. Aslam},\n    title     = {Condorcet fusion for improved retrieval},\n    booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information\n                and Knowledge Management, McLean, VA, USA, November 4-9, 2002},\n    pages     = {538--548},\n    publisher = {{ACM}},\n    year      = {2002},\n    url       = {https://doi.org/10.1145/584792.584881},\n    doi       = {10.1145/584792.584881},\n    timestamp = {Tue, 06 Nov 2018 16:57:50 +0100},\n    biburl    = {https://dblp.org/rec/conf/cikm/MontagueA02.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value step 0.1"},{"location":"fusion/#wmnz","title":"WMNZ","text":"<p>Computes Weighted MNZ as proposed by Wu et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/cikm/WuC02,\n    author    = {Shengli Wu and\n                Fabio Crestani},\n    title     = {Data fusion with estimated weights},\n    booktitle = {Proceedings of the 2002 {ACM} {CIKM} International Conference on Information\n                and Knowledge Management, McLean, VA, USA, November 4-9, 2002},\n    pages     = {648--651},\n    publisher = {{ACM}},\n    year      = {2002},\n    url       = {https://doi.org/10.1145/584792.584908},\n    doi       = {10.1145/584792.584908},\n    timestamp = {Tue, 06 Nov 2018 16:57:40 +0100},\n    biburl    = {https://dblp.org/rec/conf/cikm/WuC02.bib},\n    bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n</code></pre> Optimization Parameter Default Value step 0.1"},{"location":"fusion/#weighted-sum","title":"Weighted Sum","text":"<p>Computes a weighted sum of the scores given to documents by a list of Runs.</p> Optimization Parameter Default Value step 0.1"},{"location":"metrics/","title":"Metrics","text":""},{"location":"metrics/#aliases","title":"Aliases","text":"<p>Aliases to use with <code>ranx.evaluate</code> and <code>ranx.compare</code>.</p> Metric Alias @k .p Hits hits Yes No Hit Rate / Success hit_rate Yes No Precision precision Yes No Recall recall Yes No F1 f1 Yes No R-Precision r_precision No No Bpref bpref No No Rank-biased Precision rbp No Yes Mean Reciprocal Rank mrr Yes No Mean Average Precision map Yes No DCG dcg Yes No DCG Burges dcg_burges Yes No NDCG ndcg Yes No NDCG Burges ndcg_burges Yes No"},{"location":"metrics/#hits","title":"Hits","text":"<p>Hits is the number of relevant documents retrieved.</p>"},{"location":"metrics/#hit-rate-success","title":"Hit Rate / Success","text":"<p>Hit Rate is the fraction of queries for which at least one relevant document is retrieved. Note: it is equivalent to <code>success</code> from trec_eval.</p>"},{"location":"metrics/#precision","title":"Precision","text":"<p>Precision is the proportion of the retrieved documents that are relevant.</p> \\[ \\operatorname{Precision}=\\frac{r}{n} \\] <p>where,</p> <ul> <li>\\(r\\) is the number of retrieved relevant documents;</li> <li>\\(n\\) is the number of retrieved documents.</li> </ul>"},{"location":"metrics/#recall","title":"Recall","text":"<p>Recall is the ratio between the retrieved documents that are relevant and the total number of relevant documents.</p> \\[ \\operatorname{Recall}=\\frac{r}{R} \\] <p>where,</p> <ul> <li>\\(r\\) is the number of retrieved relevant documents;</li> <li>\\(R\\) is the total number of relevant documents.</li> </ul>"},{"location":"metrics/#f1","title":"F1","text":"<p>F1 is the harmonic mean of Precision and Recall.</p> \\[ \\operatorname{F1} = 2 \\times \\frac{\\operatorname{Precision} \\times \\operatorname{Recall}}{\\operatorname{Precision} + \\operatorname{Recall}} \\]"},{"location":"metrics/#r-precision","title":"R-Precision","text":"<p>For a given query \\(Q\\), R-Precision is the precision at \\(R\\), where \\(R\\) is the number of relevant documents for \\(Q\\). In other words, if there are \\(r\\) relevant documents among the top-\\(R\\) retrieved documents, then R-precision is:</p> \\[ \\operatorname{R-Precision} = \\frac{r}{R} \\]"},{"location":"metrics/#bpref","title":"Bpref","text":"<p>Bpref is designed for situations where relevance judgments are known to be incomplete. It is defined as:</p> \\[ \\operatorname{bpref}=\\frac{1}{R}\\sum_r{1 - \\frac{|\\text{$n$ ranked higher than $r$}|}{R}} \\] <p>where,</p> <ul> <li>\\(r\\) is a relevant document;</li> <li>\\(n\\) is a member of the first R judged nonrelevant documents as retrieved by the system;</li> <li>\\(R\\) is the number of relevant documents.</li> </ul>"},{"location":"metrics/#rank-biased-precision","title":"Rank-biased Precision","text":"<p>Compute Rank-biased Precision (RBP).</p> <p>It is defined as:</p> \\[ \\operatorname{RBP} = (1 - p) \\cdot \\sum_{i=1}^{d}{r_i \\cdot p^{i - 1}} \\] <p>where,</p> <ul> <li>\\(p\\) is the persistence value;</li> <li>\\(r_i\\) is either 0 or 1, whether the \\(i\\)-th ranked document is non-relevant or relevant, respectively.</li> </ul>"},{"location":"metrics/#mean-reciprocal-rank","title":"(Mean) Reciprocal Rank","text":"<p>Reciprocal Rank is the multiplicative inverse of the rank of the first retrieved relevant document: 1 for first place, 1/2 for second place, 1/3 for third place, and so on. When averaged over many queries, it is usually called Mean Reciprocal Rank (MRR).</p> \\[ Reciprocal Rank = \\frac{1}{rank} \\] <p>where,</p> <ul> <li>\\(rank\\) is the position of the first retrieved relevant document.</li> </ul>"},{"location":"metrics/#mean-average-precision","title":"(Mean) Average Precision","text":"<p>Average Precision is the average of the Precision scores computed after each relevant document is retrieved. When averaged over many queries, it is usually called Mean Average Precision (MAP).</p> \\[ \\operatorname{Average Precision} = \\frac{\\sum_r \\operatorname{Precision}@r}{R} \\] <p>where,</p> <ul> <li>\\(r\\) is the position of a relevant document;</li> <li>\\(R\\) is the total number of relevant documents.</li> </ul>"},{"location":"metrics/#dcg","title":"DCG","text":"<p>Compute Discounted Cumulative Gain (DCG) as proposed by J\u00e4rvelin et al..</p> BibTeX <pre><code>@article{DBLP:journals/tois/JarvelinK02,\n    author    = {Kalervo J{\\\"{a}}rvelin and\n                Jaana Kek{\\\"{a}}l{\\\"{a}}inen},\n    title     = {Cumulated gain-based evaluation of {IR} techniques},\n    journal   = {{ACM} Trans. Inf. Syst.},\n    volume    = {20},\n    number    = {4},\n    pages     = {422--446},\n    year      = {2002}\n}\n</code></pre> \\[ \\operatorname{DCG} = \\frac{\\operatorname{rel}_i}{\\log_2(i+1)} \\] <p>where,</p> <ul> <li>\\(\\operatorname{rel}_i\\) is the relevance value of the result at position i.</li> </ul>"},{"location":"metrics/#dcg-burges","title":"DCG Burges","text":"<p>Compute Discounted Cumulative Gain (DCG) at k as proposed by Burges et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/icml/BurgesSRLDHH05,\n    author    = {Christopher J. C. Burges and\n                Tal Shaked and\n                Erin Renshaw and\n                Ari Lazier and\n                Matt Deeds and\n                Nicole Hamilton and\n                Gregory N. Hullender},\n    title     = {Learning to rank using gradient descent},\n    booktitle = {{ICML}},\n    series    = {{ACM} International Conference Proceeding Series},\n    volume    = {119},\n    pages     = {89--96},\n    publisher = {{ACM}},\n    year      = {2005}\n}\n</code></pre> \\[ \\operatorname{DCG} = \\frac{2^{\\operatorname{rel}_i}-1}{\\log_2(i+1)} \\] <p>where,</p> <ul> <li>\\(\\operatorname{rel}_i\\) is the relevance value of the result at position i.</li> </ul>"},{"location":"metrics/#ndcg","title":"NDCG","text":"<p>Compute Normalized Discounted Cumulative Gain (NDCG) as proposed by J\u00e4rvelin et al..</p> BibTeX <pre><code>@article{DBLP:journals/tois/JarvelinK02,\n    author    = {Kalervo J{\\\"{a}}rvelin and\n                Jaana Kek{\\\"{a}}l{\\\"{a}}inen},\n    title     = {Cumulated gain-based evaluation of {IR} techniques},\n    journal   = {{ACM} Trans. Inf. Syst.},\n    volume    = {20},\n    number    = {4},\n    pages     = {422--446},\n    year      = {2002}\n}\n</code></pre> \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] <p>where,</p> <ul> <li>\\(\\operatorname{DCG}\\) is Discounted Cumulative Gain;</li> <li>\\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possible DCG).</li> </ul>"},{"location":"metrics/#ndcg-burges","title":"NDCG Burges","text":"<p>Compute Normalized Discounted Cumulative Gain (NDCG) at k as proposed by Burges et al..</p> BibTeX <pre><code>@inproceedings{DBLP:conf/icml/BurgesSRLDHH05,\n    author    = {Christopher J. C. Burges and\n                Tal Shaked and\n                Erin Renshaw and\n                Ari Lazier and\n                Matt Deeds and\n                Nicole Hamilton and\n                Gregory N. Hullender},\n    title     = {Learning to rank using gradient descent},\n    booktitle = {{ICML}},\n    series    = {{ACM} International Conference Proceeding Series},\n    volume    = {119},\n    pages     = {89--96},\n    publisher = {{ACM}},\n    year      = {2005}\n}\n</code></pre> \\[ \\operatorname{nDCG} = \\frac{\\operatorname{DCG}}{\\operatorname{IDCG}} \\] <p>where,</p> <ul> <li>\\(\\operatorname{DCG}\\) is Discounted Cumulative Gain;</li> <li>\\(\\operatorname{IDCG}\\) is Ideal Discounted Cumulative Gain (max possible DCG).</li> </ul>"},{"location":"normalization/","title":"Normalization","text":"<p><code>ranx</code> provides several result lists normalization strategies to be used conjunctly with the fusion methods. Normalization aims at transforming the scores of a result list into new values to make them comparable with those of other normalized result lists, which is mandatory for correctly applying many of the provided fusion methods. The normalization strategy to apply before fusion can be defined through the <code>norm</code> parameter of the functions <code>fuse</code> and <code>optimize_fusion</code> (defaults to <code>min-max</code>).</p> Normalization Strategies Alias Min-Max Norm min-max Min-Max-Inverted Norm min-max-inverted Max Norm max Sum Norm sum ZMUV Norm zmuv Rank Norm rank Borda Norm borda"},{"location":"normalization/#min-max-norm","title":"Min-Max Norm","text":"<p>Min-Max Norm scales the scores (s) of a result list between 0 and 1, scaling to 0 the minimum score (\\(s_{min}\\)) and 1 the maximum score (\\(s_{max}\\)).</p> \\[ \\operatorname{MinMaxNorm(s)}=\\frac{s - s_{min}}{s_{max} - s_{min}} \\]"},{"location":"normalization/#min-max-inverted-norm","title":"Min-Max Inverted Norm","text":"<p>Min-Max Inverted Norm scales the scores (s) of a result list between 0 and 1, scaling to 1 the minimum score (\\(s_{min}\\)) and 0 the maximum score (\\(s_{max}\\)). It is handy when distance metrics are used to compute relevance scores, i.e. when lower scores indicates higher relevance.</p> \\[ \\operatorname{MinMaxInvertedNorm(s)}=\\frac{s_{max} - s}{s_{max} - s_{min}} \\]"},{"location":"normalization/#max-norm","title":"Max Norm","text":"<p>Max Norm scales the scores (s) of a result list the maximum score (\\(s_{max}\\)) is scaled to 1.</p> \\[ \\operatorname{MaxNorm(s)}=\\frac{s}{s_{max}} \\]"},{"location":"normalization/#sum-norm","title":"Sum Norm","text":"<p>Sum Norm scales the minimum score (\\(s_{min}\\)) to 0 and the scores sum to 1. It is computed as follows:</p> \\[ \\operatorname{SumNorm(s)}=\\frac{s - s_{min}}{\\sum_s{s - s_{min}}} \\]"},{"location":"normalization/#zmuv-norm","title":"ZMUV Norm","text":"<p>ZMUV Norm (zero-mean, unit-variance) scales the scores so that their mean (\\(s_{mean}\\)) becomes zero and their variance 1.</p> \\[ \\operatorname{ZMUVNorm(s)}=\\frac{s - s_{mean}}{s_{std}} \\]"},{"location":"normalization/#rank-norm","title":"Rank Norm","text":"<p>Rank Norm transforms the scores according to the position in the ranking of the results they are associated with. In this case, the normalized scores are uniformly distributed. The top-ranked result gets a score of 1, while the bottom-ranked result gets a score of \\(\\frac{1}{|r|}\\), where \\(|r|\\) is the number of results in the ranked list.</p> \\[ \\operatorname{RankNorm(s_i)}=1-\\frac{r_i - 1}{|r|} \\]"},{"location":"normalization/#borda-norm","title":"Borda Norm","text":"<p>Borda Norm transforms the scores in a similar manner of how BordaFuse assign points to the results before fusing multiple runs. Borda Norm is defined as follows:</p> \\[ \\operatorname{BordaNorm(s_i)}= \\begin{cases}     1 - \\frac{r_i - 1}{|candidates|} &amp; \\mathit{if}\\ d \\in r \\\\     \\frac{1}{2} - \\frac{|r|-1}{2 \\cdot |candidates|} &amp; \\mathit{otherwise} \\end{cases} \\] <p>Please, refer to Renda et al. for further details.</p>"},{"location":"qrels/","title":"Qrels","text":"<p><code>Qrels</code>, or query relevance judgments, stores the ground truth for conducting evaluations. The preferred way for creating a <code>Qrels</code> instance is converting Python dictionary as follows:</p> <pre><code>from ranx import Qrels\n\nqrels_dict = {\n    \"q_1\": {\n        \"d_1\": 1,\n        \"d_2\": 2,\n    },\n    \"q_2\": {\n        \"d_3\": 2,\n        \"d_2\": 1,\n        \"d_5\": 3,\n    },\n}\n\nqrels = Qrels(qrels_dict, name=\"MSMARCO\")\n</code></pre> <p>Qrels can also be loaded from TREC-style and JSON files, from ir-datasets, and from Pandas DataFrames.</p>"},{"location":"qrels/#load-from-files","title":"Load from files","text":"<p>Parse a qrels file into <code>ranx.Qrels</code>. Supported formats are JSON, TREC qrels, and gzipped TREC qrels. Correct import behavior is inferred from the file extension: <code>.json</code> -&gt; <code>json</code>, <code>.trec</code> -&gt; <code>trec</code>, <code>.txt</code> -&gt; <code>trec</code>, <code>.gz</code> -&gt; <code>gzipped trec</code>. Use the <code>kind</code> argument to override the default behavior.</p> <pre><code>qrels = Qrels.from_file(\"path/to/qrels.json\")  # JSON file\nqrels = Qrels.from_file(\"path/to/qrels.trec\")  # TREC-Style file\nqrels = Qrels.from_file(\"path/to/qrels.txt\")   # TREC-Style file with txt extension\nqrels = Qrels.from_file(\"path/to/qrels.gz\")    # Gzipped TREC-Style file\nqrels = Qrels.from_file(\"path/to/qrels.custom\", kind=\"json\")  # Loaded as JSON file\n</code></pre>"},{"location":"qrels/#load-from-ir-datasets","title":"Load from ir-datasets","text":"<p>You can find the full list of the qrels provided by ir-datasets here.</p> <pre><code>qrels = Qrels.from_ir_datasets(\"msmarco-document/dev\")\n</code></pre>"},{"location":"qrels/#load-from-pandas-dataframes","title":"Load from Pandas DataFrames","text":"<pre><code>from pandas import DataFrame\n\nqrels_df = DataFrame.from_dict({\n    \"q_id\":   [ \"q_1\",  \"q_1\",  \"q_2\",  \"q_2\"  ],\n    \"doc_id\": [ \"d_12\", \"d_25\", \"d_11\", \"d_22\" ],\n    \"score\":  [  5,      3,      6,      1     ],\n})\n\nqrels = Qrels.from_df(\n    df=qrels_df,\n    q_id_col=\"q_id\",\n    doc_id_col=\"doc_id\",\n    score_col=\"score\",\n)\n</code></pre>"},{"location":"qrels/#load-from-parquet-files","title":"Load from Parquet files","text":"<p><code>ranx</code> can load <code>qrels</code> from Parquet files, even from remote sources. You can control the behavior of the underlying <code>pandas.read_parquet</code> function by passing additional arguments through the <code>pd_kwargs</code> argument (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html).</p> <pre><code>qrels = Qrels.from_parquet(\n    path=\"/path/to/parquet/file\"\"\",\n    q_id_col=\"q_id\",\n    doc_id_col=\"doc_id\",\n    score_col=\"score\",\n    pd_kwargs=None,\n)\n</code></pre>"},{"location":"qrels/#save","title":"Save","text":"<p>Write <code>qrels</code> to <code>path</code> as JSON file or TREC qrels format. File type is automatically inferred form the filename extension: <code>.json</code> -&gt; <code>json</code>, <code>.trec</code> -&gt; <code>trec</code>, <code>.txt</code> -&gt; <code>trec</code>, <code>.parq</code> -&gt; <code>parquet</code>, <code>.parquet</code> -&gt; <code>parquet</code>. Use the <code>kind</code> argument to override the default behavior.</p> <pre><code>qrels.save(\"path/to/qrels.json\")     # Save as JSON file\nqrels.save(\"path/to/qrels.trec\")     # Save as TREC-Style file\nqrels.save(\"path/to/qrels.txt\")      # Save as TREC-Style file with txt extension\nqrels.save(\"path/to/qrels.parq\")     # Save as Parquet file\nqrels.save(\"path/to/qrels.parquet\")  # Save as Parquet file\nqrels.save(\"path/to/qrels.custom\", kind=\"json\")  # Save as JSON file\n</code></pre>"},{"location":"report/","title":"Report","text":"<p>A <code>Report</code> instance is automatically generated as the results of a comparison. A <code>Report</code> provides a convenient way of inspecting a comparison results and exporting those il LaTeX for your scientific publications. By changing the values of the parameters <code>rounding_digits</code> (int) and <code>show_percentages</code> (bool) you can control what is shown on printing and when generating LaTeX tables.</p> <p><pre><code>from ranx import compare\n\n# Compare different runs and perform statistical tests\nreport = compare(\n    qrels=qrels,\n    runs=[run_1, run_2, run_3, run_4, run_5],\n    metrics=[\"map@100\", \"mrr@100\", \"ndcg@10\"],\n    max_p=0.01  # P-value threshold\n)\n\nprint(report)\n</code></pre> Output: <pre><code>#    Model    MAP@100     MRR@100     NDCG@10\n---  -------  ----------  ----------  ----------\na    model_1  0.3202\u1d47     0.3207\u1d47     0.3684\u1d47\u1d9c\nb    model_2  0.2332      0.2339      0.239\nc    model_3  0.3082\u1d47     0.3089\u1d47     0.3295\u1d47\nd    model_4  0.3664\u1d43\u1d47\u1d9c   0.3668\u1d43\u1d47\u1d9c   0.4078\u1d43\u1d47\u1d9c\ne    model_5  0.4053\u1d43\u1d47\u1d9c\u1d48  0.4061\u1d43\u1d47\u1d9c\u1d48  0.4512\u1d43\u1d47\u1d9c\u1d48\n</code></pre> <pre><code>print(report.to_latex())  # To get the LaTeX code\n</code></pre></p>"},{"location":"run/","title":"Run","text":"<p><code>Run</code> stores the relevance scores estimated by the model under evaluation. There is no constraint on the score values, i.e., zero and negative scores are not removed.  The preferred way for creating a <code>Run</code> instance is converting a Python dictionary as follows:</p> <pre><code>from ranx import Run\n\nrun_dict = {\n    \"q_1\": {\n        \"d_1\": 1.5,\n        \"d_2\": 2.6,\n    },\n    \"q_2\": {\n        \"d_3\": 2.8,\n        \"d_2\": 1.2,\n        \"d_5\": 3.1,\n    },\n}\n\nrun = Run(run_dict, name=\"bm25\")\n</code></pre> <p><code>Runs</code> can also be loaded from TREC-style and JSON files, and from Pandas DataFrames.</p>"},{"location":"run/#load-from-files","title":"Load from Files","text":"<p>Parse a run file into <code>ranx.Run</code>. Supported formats are JSON, TREC run, gzipped TREC run, and LZ4. Correct import behavior is inferred from the file extension: <code>.json</code> -&gt; <code>json</code>, <code>.trec</code> -&gt; <code>trec</code>, <code>.txt</code> -&gt; <code>trec</code>, <code>.gz</code> -&gt; <code>trec</code>, <code>.lz4</code> -&gt; <code>lz4</code>. Use the argument <code>kind</code> to override the default behavior. Use the argument <code>name</code> to set the name of the run. Default is <code>None</code>.</p> <pre><code>run = Run.from_file(\"path/to/run.json\")  # JSON file\nrun = Run.from_file(\"path/to/run.trec\")  # TREC-Style file\nrun = Run.from_file(\"path/to/run.txt\")   # TREC-Style file with txt extension\nrun = Run.from_file(\"path/to/run.gz\")    # Gzipped TREC-Style file\nrun = Run.from_file(\"path/to/run.lz4\")    # lz4 file produced by saving a ranx.Run as lz4\nrun = Run.from_file(\"path/to/run.custom\", kind=\"json\")  # Loaded as JSON file\n</code></pre>"},{"location":"run/#load-from-pandas-dataframes","title":"Load from Pandas DataFrames","text":"<p><code>ranx</code> can load <code>runs</code> from Pandas DataFrames. The argument <code>name</code> is used to set the name of the run. Default is <code>None</code>.</p> <pre><code>from pandas import DataFrame\n\nrun_df = DataFrame.from_dict({\n    \"q_id\":   [ \"q_1\",  \"q_1\",  \"q_2\",  \"q_2\"  ],\n    \"doc_id\": [ \"d_12\", \"d_25\", \"d_11\", \"d_22\" ],\n    \"score\":  [  0.5,    0.3,    0.6,    0.1   ],\n})\n\nrun = Run.from_df(\n    df=run_df,\n    q_id_col=\"q_id\",\n    doc_id_col=\"doc_id\",\n    score_col=\"score\",\n    name=\"my_run\",\n)\n</code></pre>"},{"location":"run/#load-from-parquet-files","title":"Load from Parquet files","text":"<p><code>ranx</code> can load <code>runs</code> from Parquet files, even from remote sources. You can control the behavior of the underlying <code>pandas.read_parquet</code> function by passing additional arguments through the <code>pd_kwargs</code> argument (see https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html). The argument <code>name</code> is used to set the name of the run. Default is <code>None</code>.</p> <pre><code>run = Run.from_parquet(\n    path=\"/path/to/parquet/file\"\"\",\n    q_id_col=\"q_id\",\n    doc_id_col=\"doc_id\",\n    score_col=\"score\",\n    pd_kwargs=None,\n    name=\"my_run\",\n)\n</code></pre>"},{"location":"run/#save","title":"Save","text":"<p>Write <code>run</code> to <code>path</code> as JSON file, TREC run, LZ4 file, or Parquet file.  File type is automatically inferred form the filename extension: <code>.json</code> -&gt; <code>json</code>, <code>.trec</code> -&gt; <code>trec</code>, <code>.txt</code> -&gt; <code>trec</code>, and <code>.lz4</code> -&gt; <code>lz4</code>, <code>.parq</code> -&gt; <code>parquet</code>, <code>.parquet</code> -&gt; <code>parquet</code>. Use the <code>kind</code> argument to override this behavior.</p> <pre><code>run.save(\"path/to/run.json\")     # Save as JSON file\nrun.save(\"path/to/run.trec\")     # Save as TREC-Style file\nrun.save(\"path/to/run.txt\")      # Save as TREC-Style file with txt extension\nrun.save(\"path/to/run.lz4\")      # Save as lz4 file\nrun.save(\"path/to/run.parq\")     # Save as Parquet file\nrun.save(\"path/to/run.parquet\")  # Save as Parquet file\nrun.save(\"path/to/run.custom\", kind=\"json\")  # Save as JSON file\n</code></pre>"},{"location":"run/#make-comparable","title":"Make comparable","text":"<p>It adds empty results for queries missing from the run and removes those not appearing in qrels.</p> <pre><code>run.make_comparable(qrels)\n</code></pre>"},{"location":"stat_tests/","title":"Statistical Tests","text":"<p><code>ranx</code> provides two statistical tests that can be used when comparing different runs:  </p> <ul> <li>Fisher's Randomization Test</li> <li>Two-sided Paired Student's t-Test</li> </ul> <p>Please, refer to Smucker et al. for additional information on statistical tests for Information Retrieval.</p> <p>To use the Fisher's Randomization Test, pass <code>stat_test=\"fisher\"</code> to compare.</p> <p>To use the Two-sided Paired Student's t-Test, pass <code>stat_test=\"student\"</code> to compare.</p>"}]}